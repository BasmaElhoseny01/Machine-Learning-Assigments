{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook, we will learn how to apply Logistic regression for predicting the cooling load requirements (Y2) of buildings as a function of building parameters (Xs).\n",
    "\n",
    "The attached dataset is taken from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency).\n",
    "\n",
    "To run this code, you will need the following python packages:\n",
    "* numpy\n",
    "* pandas\n",
    "* openpyxl\n",
    "* scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the dataset using pandas\n",
    "df = pd.read_excel(\"Energy_Efficiency.xlsx\", engine = 'openpyxl')\n",
    "# Remove any unnamed columns (might occur due to difference in pandas readers)\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "# Remove any row with NaNs\n",
    "df = df.dropna(how='all')\n",
    "# Drop Y1 (as we only consider Y2 for classification)\n",
    "df = df.drop('Y1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we will split the data frame into a training and testing splits with a 70% / 30% ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42) # Random is fixed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>15.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>19.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "      <td>46.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>30.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.76</td>\n",
       "      <td>661.5</td>\n",
       "      <td>416.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>33.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.86</td>\n",
       "      <td>588.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>27.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.71</td>\n",
       "      <td>710.5</td>\n",
       "      <td>269.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>14.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>30.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>29.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1     X2     X3      X4   X5  X6    X7  X8     Y2\n",
       "334  0.62  808.5  367.5  220.50  3.5   4  0.25   1  15.77\n",
       "139  0.64  784.0  343.0  220.50  3.5   5  0.10   2  19.30\n",
       "485  0.90  563.5  318.5  122.50  7.0   3  0.25   5  32.00\n",
       "547  0.79  637.0  343.0  147.00  7.0   5  0.40   1  46.94\n",
       "18   0.79  637.0  343.0  147.00  7.0   4  0.00   0  30.93\n",
       "..    ...    ...    ...     ...  ...  ..   ...  ..    ...\n",
       "71   0.76  661.5  416.5  122.50  7.0   5  0.10   1  33.67\n",
       "106  0.86  588.0  294.0  147.00  7.0   4  0.10   2  27.36\n",
       "270  0.71  710.5  269.5  220.50  3.5   4  0.10   5  14.26\n",
       "435  0.98  514.5  294.0  110.25  7.0   5  0.25   4  30.12\n",
       "102  0.90  563.5  318.5  122.50  7.0   4  0.10   2  29.36\n",
       "\n",
       "[537 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.760354</td>\n",
       "      <td>674.867784</td>\n",
       "      <td>318.636872</td>\n",
       "      <td>178.115456</td>\n",
       "      <td>5.201117</td>\n",
       "      <td>3.500931</td>\n",
       "      <td>0.235940</td>\n",
       "      <td>2.854749</td>\n",
       "      <td>24.287505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.104790</td>\n",
       "      <td>87.758133</td>\n",
       "      <td>43.619254</td>\n",
       "      <td>44.839207</td>\n",
       "      <td>1.750948</td>\n",
       "      <td>1.106502</td>\n",
       "      <td>0.134118</td>\n",
       "      <td>1.544532</td>\n",
       "      <td>9.505775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.660000</td>\n",
       "      <td>612.500000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>21.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>759.500000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>32.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4          X5          X6  \\\n",
       "count  537.000000  537.000000  537.000000  537.000000  537.000000  537.000000   \n",
       "mean     0.760354  674.867784  318.636872  178.115456    5.201117    3.500931   \n",
       "std      0.104790   87.758133   43.619254   44.839207    1.750948    1.106502   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.500000    2.000000   \n",
       "25%      0.660000  612.500000  294.000000  147.000000    3.500000    3.000000   \n",
       "50%      0.740000  686.000000  318.500000  220.500000    3.500000    3.000000   \n",
       "75%      0.820000  759.500000  343.000000  220.500000    7.000000    4.000000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.000000    5.000000   \n",
       "\n",
       "               X7          X8          Y2  \n",
       "count  537.000000  537.000000  537.000000  \n",
       "mean     0.235940    2.854749   24.287505  \n",
       "std      0.134118    1.544532    9.505775  \n",
       "min      0.000000    0.000000   10.940000  \n",
       "25%      0.100000    2.000000   15.500000  \n",
       "50%      0.250000    3.000000   21.160000  \n",
       "75%      0.400000    4.000000   32.920000  \n",
       "max      0.400000    5.000000   48.030000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median value of the target: 21.16\n",
      "Percentage of 'high load' samples: 49.906890130353815 %\n"
     ]
    }
   ],
   "source": [
    "# Now we will extract the models input and targets from both the training and testing data frames\n",
    "def extract_Xy(df):\n",
    "    df_numpy = df.to_numpy()\n",
    "    return df_numpy[:, :-1], df_numpy[:, -1]\n",
    "\n",
    "X_train, y_train = extract_Xy(df_train)\n",
    "X_test, y_test = extract_Xy(df_test)\n",
    "\n",
    "y_median = np.median(y_train)\n",
    "print(\"Median value of the target:\", y_median)\n",
    "\n",
    "# Since we will treat this as a classification task, we will assume that\n",
    "# the load is \"high\" (y = True) if its compressive ratio is higher than the median\n",
    "# otherwise, it is assumed to be \"low\" (y = False)\n",
    "y_train = y_train > y_median\n",
    "y_test = y_test > y_median\n",
    "\n",
    "# Now ~50% of the samples should be considered \"high\" and the rest are considered \"low\"\n",
    "print(f\"Percentage of 'high load' samples: {y_train.mean() * 100} %\")\n",
    "\n",
    "# Also, lets standardize the data since it improves the training process\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_mean)/(1e-8 + X_std)\n",
    "X_test = (X_test - X_mean)/(1e-8 + X_std)"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAADqCAYAAAARfBpXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABzaSURBVHhe7d0JuJRVGcDxY2YailpgRpZGpqVZmCYGbkiYiZqWlWnupKkFmgspYqGGZamPaZuWYYukYAmWS1YumELmQ7hEBlJqYimiiUsq5jT/w/ddh8vcZebOcs/c/+955rlzv7lcZu58871nec97VisUBUmSEvCa7KskSb2eQUuSlAyDliQpGQYtSVIyDFqSpGQYtCRJyTBoSZKSYdCSJCXDoCVJSoZBS5KUDIOWJCkZBi1JUjIMWpKkZBi0JEnJMGhJkpJh0JIkJcOgJUlKhkFLkpQMg5YkKRkGLUlSMgxakqRkGLQkSckwaEmSkmHQkiQlw6AlSUqGQUuSlAyDliQpGQYtSVIyDFqSpGQYtCRJyTBoSZKSYdCSJCXDoCVJSoZBS5KUDIOWJCkZBi1JUjIMWpKkZBi0JEnJMGhJkpJh0JIkJcOgJUlKhkFLkpQMg5YkKRkGLUlSMgxakqRkGLQkSckwaEmSkmHQkiQlw6AlSUqGQUuSlAyDliQpGQYtSVIyDFqSpGQYtCRJyTBoSZKSYdCSJCXDoCVJSoZBS5KUDIOWJCkZBi1JUjIMWpKkZBi0JEnJMGhJkpJh0JIkJcOgJUlKhkFLkpQMg5YkKRkGLUlSMgxakqRkGLQkSckwaEmSkmHQkiQlw6AlSUqGQUuSlAyDliQpGQYtSVIyVisUZfelTp199tnh9ttvz76T1AjXXnttdk8waKnbCFqSGmvChAnZPcGgpW7Lg5YfIknN4pyWJCkZBi1JUjIMWpKkZBi0JEnJMGhJkpJh0JIyf/rTn8Khhx4aJk+eHJ566qnwxBNPhPPPPz/su+++YezYsWHRokXZT0pqFoOWVLRkyZJw6623xoB11113hfHjx4ef/OQn4YgjjghXX311eNe73hXOOOOMsGzZsuxfSGoGg5ZUdN9994Vtt902rL766uGll14Kr3/968PRRx8d1l9//bDaaquFAQMGhGeeeSa8/PLL2b+Q1AwGLalo8803j0HrscceCwsXLgz7779/6NevX3zsf//7X1iwYEHYYIMNwute97p4TFJzGLSkoo022iisu+664a9//WvYZJNNwsYbb5w9EsLjjz8e7rzzzjBkyJCwzjrrZEclNYNBS8owLHjPPfeEwYMHhze84Q3Z0RD+9re/hQcffDB88IMfzI5IahaDlnoFEhwYmmsmnsP8+fNX6lExNEiCBkOHm222WZgzZ0646qqr4mOSGs+gpaZ65ZVXwqxZs8IBBxwQLrvssuxoczzyyCNxeHDrrbfOjoTw9NNPh3nz5oXtt98+rLnmmnFrlqFDh2aPSmo0g5Yaip4LQ3DTp08Pp59+ethll13CXnvtFW677bbsJ5qHdVhbbLFF2HTTTbMjIay33nphxIgRMVjxfIcNG7bSfJekxjJoqaGWL18errnmmnD55ZeH559/Phx33HFh+PDh2aPNtffee4epU6eGN7/5zdmREFPgx40bF77//e/HdVq95blKfZVBSw211lprhYkTJ4YZM2aE8847L4wcOTKuieoNSGdfe+21s+9exTotMgt7y/OU+jKDliQpGQYtSVIyDFqSpGQYtCRJyTBoSZKSYdCSJCXDoCVJSsZqhaLsvtSps88+O36dMGFC/FoLTz75ZBgzZkz4/e9/H0477bTwpS99KXukOv/617/CL3/5y1j8ttne8pa3xF2PKf8kqTYMWuq2FIIWW+bvs88+4dlnn82OrLDddtuF/v37Z99Vjg0g+d2VYLdjdj+mNJSk2jBoqdtSCFrsLMzzPPfcc7MjK4wdOzZ8+ctfrlmvh54cVeH/8Y9/hLlz58ZK8LfccssqwZLn8vnPfz5W1ZDUc85pqaW89rWvjdvk77nnntmRFaZMmRJrHtaqjUbJp4EDB8Ye3Oc+97lYs5B9ty699NKwzTbbZD8Vwm9/+9umb7kitRKDllrOm970pliI921ve1t2JMQe0AUXXBC3za8Xhh8/+clPht/85jfhu9/9btxI8uabb467HkuqDYOWWhL7XzEcWOree+8NkyZNitvn1xNDkAcddFCYOXNm+MAHPhCuv/76WNFeao/h7MWLF4f77rsvPPfcc9lRdWb14od4UnZf6lS+59VOO+0Uv1aLYTQ2W3z44YfDHXfcEX7961/H+aEBAwaE9ddfP27G+Oijj8aeCsNw1WAO6R3veEfcxJE5p9zChQvj/8Ow3mteU982G1ucsEX/lVdeGd773vfGbMKucBGbNm1afJ4mcKSFc/jrX/96fJ85x7pCr//UU0+N86IMI3OfzwTnpjsKdIJEDKk7Jk+eHG89VfxgF/r379/pbfjw4YXihzr7F9V76KGHCrvttttKv3vQoEGFYqDMfqK+XnnllcIVV1xRmDBhQuGFF17Ijpa3fPnywsUXX1w48cQTC8VWd3ZUKSk27AqjR48u/OUvf8mOlFfs7cdz4rHHHsuOrPi3W265ZXz/iz3z7KjaM3tQ3VaP7MFG+MMf/hCTJf75z39mR0Ictvve974X09LrjXT5q6++OowaNarD3hYfw6uuuir87Gc/i8+rs14Zuz9fdNFFMVuxI0ceeWRbMgpDT2xgyQac7THvd9ZZZ8Uernoufx/ZNJR5zY7Or+uuuy589rOfjZ+lY445Jm42+t///jf2tniM3vbWW2+d/bRKOaellscQHReIUnfddVf4zne+05C5pmLvLhxyyCGdBqJiyzycf/754aijjurWMOLb3/72sMsuu4Riyzz8+c9/DjfddFO8MSz6oQ99KAwZMiT7yRDWWGONONSY/xyJIf369Qs77rhj/Fmen2qDvz+Nhfe85z3hkksu6fT8IjmIocG8McGQ4KBBg8K///3vuiYMJY+eltQdtRoebIZly5YVigFhpWFCbhdccEHh5Zdfzn6qORgKOu644+Lz43lWgiHHk08+ue31zJw5M3tkZY888khh9913LxR7YPG+6qvYKCpsttlmhSuvvDI7sjLet7lz5640PMh5OGnSpPg+FhsX2VG1Z09LfULxQhBOOOGEOCxYitbw7Nmzs++a449//GMcDqKFzvOsBJmKo0ePDuuss078vlymIt/Ti6P1T9r/RhttlD2ieqFnSy/28ssvjz2n9njf3v/+98flGbklS5bEpCHez6222io7qvYMWuozmF849thj2y7wYJ6LjC+yFZvhxRdfjHMYXMDIGqsGcx+77rprvE9lkdKhJbIRL7744vDEE0/EaiMMC6r++Dvvscce3V6nV+xAxHlPMmep3rLBBhtkj6g9g5b6FArYHn/88dl3K8yaNSucd955TVlL9fe//z1e2IYOHbpSq7sSJFHsvvvu8T6tehJPuAgSsKjUwfzdN7/5zap/v6pDb4ulDswjkmTRmdtvvz0WeiYJZ4cddsiOqhyDlvoUyjwddthhq5R5+vnPfx6uvfbaeLFvJBY8s26NCxwZZNUiKSMf+mTND8GLlvuPf/zjuMjagNV4JFUwJEuSDT3djvD+k2144YUXxsaLOtdnFhdzMaISwj333BMefPDBuIiP2/333x+HZ1jkOmfOnLjolcwsF/etqlaLi5tt7bXXDptuumkcSmNBKCiAS3bd8OHD48WmEUhdp3VNBhn1ElmMXC2Go1iQTCV6zm+qK9DC50LYiLT+3i6f1/vd734XAwMZlaW4PhBcGCYmwNeiwDH/B7/ziiuuCHvvvXfYeOONs0detWjRoti7mjhxYjwnQc+Yxsw73/nO+L3aKb5ZLY8Fpsccc0xcVJpnWXV2u+GGG7J/2bqKvYrsXvelnD3YHot+p02btso58elPf3qljK56evrppwsHH3xwodgIKBQvXtnR6t18881tr4dFqixW1QrFBknM5vvoRz9aWLJkSXb0VfPnzy8Ue6qFfffdt7B06dLsaM9NnTo1vh98bY/z7LTTTissXrw4O7LClClTCnfeeWf2ndpr+cXFjBWPGzcutkJHjBgRx4vzxZ5MwtMK/fjHPx7WWmut+POskSme2DXtadGi/tWvfhVb1D3FEBLPj95gT1DYdfr06dl33ZPq4uKO0Po+/fTTww9+8IPsyAq0esk0ZCixnjj/jjjiiJgYQnX4N77xjdkj1fnPf/4Tt0HhXOO8di+vV/3oRz+Kc5ksui42vNo+7zmyN1nLx9+PhdjVlg9rj1GcYkNolW138nNv3rx5K2WMMg/J/BfnJGXItKqWntNiuI/FmpygN954Yyxgyonz1a9+NVbi3nnnnWNtOrJ8vvjFL8bb/vvv32HAIr6zIJATq1LVBsG3vvWtca4iv5FlVnqSV2PGjBnx9bNyvy9jSO3EE0+M50Ep0sL5+9TbCy+8EC9evJ+1CJBsgcLOzWCehOHPStuknNtdJaQUe6lxWJWvKeDvzDAdmGNqH7DyoWEUe2NdBqxKXn/eEGFqgueRY2kCgYnhXIZx8xtJQeuuu64VSjpDT6sVMQRw4IEHxuGS4oc3O/qq4oezbSFfRwsA22PYkN9XvNhnR9K0zz77xNdNjbRKtNLwYKm85ht/k/y26667Fu6///7sJ+ojH7JiYXExUGRHq8NQE+c7deuo28hrYCisGMSyn+gaz4F/P2rUqMLDDz+cHV0V9RH5HKQyhPXoo4/Gc53nPHv27Ozoq/LHeS94T7pSyevn/+O9qMV7rBVatqdFt5xhErZep4xPLdBC+8hHPhLe9773ZUfSQ4YcySbIEyvqjZZjLW71MmzYsNgjL8VkOBPkXaUq9wQt73ILTytF75+1ZvTEzzzzzLaUaVrxJBpVgr/zxz72sU6TUdZbb72w3377hU022SQ70j1kMrLkoCc3Xl+l78lDDz0Ue1KMUpR7zvRQmT6gl7XhhhtmRztW7etXjWTBq6U888wzsVwNLZyOelHd+ZlWVAy6sbfE6+Z24403Zo90rVV7WqCq+tixY9v+LrSkSdQgYaNeatEK53mfcMIJhfHjx7dVhi9NyKDEEyWDegMSDmbNmtWjGwkrlb4nl156afxb8HcqBrzs6Ku6erwn7GnVXkv2tBg/poVJCikFRct56qmnYmIEacabb755drS1sX6HlF4m/3O0frVifotJeBaDkhjxrW99K3ziE5+oSepzR5jvYI6lWlTToDeIr3zlK23VLkorZLBwmQXMvQFJTiyX6MmN5IRK3hN6ZSxzQbn5rK7mu9T7tGTQYpKUYRfWRXS09oXhAmq+UR+ss6DFZCsffC70xdZSXANTKT4YTPi3H+6q5sYHn7Um1fjGN74RxowZE/8mbLCIa665Jn7t64oNuLhujwy8Yu8kDpHVM2CB389GlAxfVTrkRcIEW18wjNm+PFNphYzuJGTwu0jKIaOULLtyiUYESH7m8MMPj4uVO1ss25vwfjL0h3eVWa9G45UGLg2VvIFLo5e/W6mevn6GHQ2ItdGSQYvFo9TuYvdQClO2R8o7cztcvElHLf3Al+KDzuJPTmIyffh56riRbVQJTlYyFgl4Pb3Nnz8/jBw5MvvN3cfeS8x98KHDpz71qfgV9MD6OpZGkBLNnCV7b9U73R00HAYOHBjPJy6K3cV5yZINCv3SEClX7aJ9hQzmbcrhd9Fw4XmQ6s15nmfS5QhiP/zhD+Nniu1cuNCzG3MK8vksetDl/k754/SyKCTM3+Oyyy4Ld999d/YTPXv9NIRAg7PejaC+oiWDFicX65iWLl26ysWAk/KGG26ISRpsvtZZna8HHnggBiwmXbmwMLTG1+6kurZHyjvDQT290YquZpt4as/RW+yfpcvnwQt9vbdFq5rWMxf5RhaVpXHFaMCTTz4ZG1Ldwfn805/+NFZ34DmziWM5JFJsv/328T4jBR0VbSWYMUxOMgrnOxfo0oLCYESCngJLA55//vl4IS5N3+7NaOTRWON5t78WcIy1ijz+7ne/OwZulsBQpYIgl+vJ66dyOwYPHhy/qudaMmhxYWd4hx1bGRrJgwwnLVsFsNCP6lVUU+6sRU0PiXVb/AzDAQwjpDruTc+ydC6LoZB8iLAvz2vRKOFc4KLU6KKyNGRoXDGnsnjx4uzoquhhk+lJa59s2C984QuxQUbpMR5rj8YVgYqvOQIdw8r8m/xCCgIU5zgotEsjrv3Cdf429EDpKVCFnOzTbbbZJnu09yqdzyIQEbjzawGfZzIuCdSlQZoeN43e0oW91b5+AhvzicytlyvhpCoVex4tafny5YViK6pQPLkKI0eOjGuTuM9alnnz5lWcgUQWUPFEbqnyKmPGjImZTdy6k0XYatmDeeZdI9ZkdeSmm26Kf/9vf/vb2ZGVkXFG5ln+PrW/tc98femllwoTJ04s+7P5rVy2bF7GqKNNJHP8251qVHaq3vL1V2RS8pz5yrWA9Wt8z/WBvy8bgfLZ5jF+fsGCBdlvWFUlr5+1ovxflOqiZJdqo2WDVq7Yu4o7tS5cuLDw7LPPZkcrx0WFE5oPQqtgUW1+ITvppJOyox1rpaBFwDrjjDOaXqOPxb9c2Fh+wTKMZin2tmPwLvbcsiOr4rN0yimnNP25dhcNTQJV/rnl83/33XfHBgqvJUcDlqBCzcHS4+1V+vrz3Ys7apCoOi05PFiKkixMsFIxmTmEajDMkC8+bKXyKjvuuGPbECGVqPsK5m1IqKHeH8kHzdy/iKGn3Xbbra06ezOQYUfZKjJpKRvGHlzUxGuPeTfm/3ryWWqkfD6L+SrOc54zhQHIIiwt1cSwH4kSzBl3VsKpktdfvLbGRBkWIleTOKWOtXzQqgXGv5l34ISvZSHd3oALJpiA7gtZhFxMyLwrtn7DueeeG5NsmpnVxf9NejrJH2R48vwajTkuAibbsnCuM8/Hhb09CvwyT8wOy838m3VHV+uzqlHJ6yfBhc8TVTz6yjrQRjFodQPZQgSt0oyiVsHEfo6iwq2OiXYSL0hmaMRaLLz44otxPRWp0uXQcj/ooIPiRpQkBjQaIxGUOiNZh0DO3k/lGmckILDsI4XyRfn6LJIsavW57e7rp+FBhjI9M5bU9GRzT63KoNUNDJVQt4yhk1bDBSofIuSD1srIjmMNViPXYoGANWXKlFU2HswROAlaW221VQxc5Rb31hO9vHPOOSduyfK1r32tbTPCUiz1oOfCUFsKuyCT5ZfXG6zF57aS179gwYKYrcmSGrcXqT2DVhkMkXDx4Ctj4iw0ZGiQxcqtKB8iZKFlqw4R5muxmL869dRTG7YWi6E2hiJZM9V+/VOp/v37h/Hjx8cGEsOXjR4mJIAzX1sayBlhYBEta5NIseeiPWTIkE5fR2/BkCef3W233bbqz201r5+fveSSS8Jee+3VsJ58X2PQKoN1S5RdYo0F+96wxuOwww6r2cZwvc2HP/zh7F5rDhGWrsWiMgmLSBuBiyZrgbj4sXi3K/RwWCtGFRaGMZuJNUbUX6QCBHM5JOqQrECppxTQm6a3RUOlms9tNa+fHjIBiwXfxx57bMN68n1Ny+9cXA1auhQipWvPiUcLuNUXBzJOz5wLr7lc5hh6unMxv596efx+kg8OPPDA2ONhzodFr9OmTYubGDJxffDBB8fhsp62VOkt83yZK+I9LZdgUA8kNrCIncorRx99dDjrrLPKlhQrh6Go5cuXNzVDj922+XuxMSEZcFR04ELcqIDfbNW8fhYu01ChJ1ZN1Rp1j0FLEVuNEzTwi1/8om3IsFRPghbDJrR680BENZKTTjopzuWQdk4VBoZT+LBzgaenQR28PffcM/sNleP/ZEtz5upIcSfFv94o1Mw293mNOtBKHz16dLwvqWdsDiiq9xAhJXTotR5wwAExtZqMLuYLCF4ETIbv2FKDuUPqItJiZX6t0urnuXwtFgGL7fPrtRaL58dcIIGJniPlfRiCzAMWW4SkUPJISoU9LUWs0yJgdDZEWG1Pi+Eu9nsiMHIRpxe1xx57xGGUCy+8MO5bVYq6eaTis8aFJAbmEirBKc02EmwlM3To0FjotBYT4rwOknLowTHnwcQ8wbUzFOA95ZRTTHuWasSgpTZdDRFWG7SoYs78Dll7zA0ydMZ9CviSYt1+TRB7OrFNyJFHHhkmT55c0cJQTudZs2bF+Qcm0JuJNT1koZLBJqk2HB5Um3oNEVIeh0ltAlZeEgvlKoz0tJLBvffe2ysCFqicUG7Nk6Tq2dNSG3pEzCtRTYAFmdRuK9XT7EEQTOhhMQ9Er47/rxTHmdMiXbnc45L6NntaakOPaNSoUfE+QaMeC41Zs8SmehQfZt1Ue8wZUUGCskJWE5DUnkFLKymdx6Lyd63lCR555e1SDA2yLgZUkCABgzVclOPpKuFBUt9g0NJK6GlRUgi13oafoMTuzyg3X8WCXDILWQCcb+fA/BcLkhtdj09S72TQ0krYajzvbbFQtpZDhFSnIGiRVVdu7RJzaNQIpMgppXCoSkBvb6eddmqpfcwkVc+gpVXk81pgu4paWbx4cdzihaoYBKX28kxCsgy5T1V29i+ijpwkwaClVdDTyuvezZgxI36tBZIwmJti3VK5ntOIESPidg6sbWLxMWWczjzzzCS2wpDUGKa8q6xDDjmkLWBNnz49Frjtaco781IUxB00aFCHFbApOrps2bJ4n4oZtayUze+mmgUBuR5bRuTJIils3SGlyp6WyiodIpw5c2Z2r2cIQAwLdhaIKJhLL6z93k49xfzYRRddFOfSSKuvFdp8rG+jmjt7KDGcKal+DFoqi6CVzzHVKmg1G6+HorZsfVILLMIeN25crFbP2rO5c+dmj0iqF4cH1aHPfOYzsQeBqVOntpVX6klFjFY1Z86cWAaL8lcsjJZUH/a01KHSIcJaJmRIUrUMWuoQWYT5VuW1XmjcKAwk3HrrrXFY8JxzzomJGJLS5fCgOsWmjflaLfaFQkrDg6z1ImORBcpHHXVUOPTQQ8N+++0XHyNLkcQJkjS6a8CAAWGLLbZYJfvQ4UGpMexpqVOlQ4QLFizI7qWBXtUdd9wRe4wsbF60aFEMOrnly5dXFLDARpCV/htJtWNPS51iQTDbg7DGit13Tz755GR6WhTbpVr94MGDY/1CivGyYLk0cNWKPS2pMexpqVOUVMp7W6n1MNZcc824CePjjz8eayhShJftVySly6ClLpUOEaaIIr0PPPBAGDZs2EpzUUuXLg233XZbRTe2VrHivNQ8Bi11iTmhepQ9agSGCK+77rqw8847xwSKW265JVx//fXxsTXWWCMOeVaCbMpK/42k2jFoqUvMCaXa23ruuefinlzbbbdd/J4NJfP7bDI5fPjwmFnY3duWW25ZNoCToAGTNKT6MmipW0aPHp3dSwuBiV7W7Nmzw6RJk8IOO+wQBg4cmD3aM2xqefzxx8f/g7qDoDo933OcxyXVltmD6hYuwBtuuGFcq5VaGae8cny/fv3aFktLSpM9LXULxWYPP/zw7Lu05JXjDVhS+uxpSZKSYU9LkpQMg5YkKRkGLUlSMgxakqRkGLQkSckwaEmSkmHQkiQlw6AlSUqGQUuSlAyDliQpGQYtSVIyDFqSpGQYtCRJyTBoSZKSYdCSJCXDoCVJSoZBS5KUDIOWJCkZBi1JUjIMWpKkZBi0JEnJMGhJkpJh0JIkJcOgJUlKhkFLkpQMg5YkKRkGLUlSMgxakqRkGLQkSckwaEmSkmHQkiQlw6AlSUqGQUuSlAyDliQpGQYtSVIyDFqSpGQYtCRJyTBoSZKSYdCSJCXDoCVJSoZBS5KUDIOWJCkZBi1JUjIMWpKkZBi0JEnJMGhJkpJh0JIkJcOgJUlKhkFLkpQMg5YkKRkGLUlSMgxakqRkGLQkSckwaEmSkmHQkiQlw6AlSUqGQUuSlAyDliQpGQYtSVIyDFqSpGQYtCRJyTBoSZKSYdCSJCXDoCVJSoZBS5KUDIOWJCkRIfwf1UgjIzoZF8IAAAAASUVORK5CYII="
    },
    "image-3.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For The Normalization Step in the cell above ðŸ”\n",
    "The mathematical equations behind the code for standardization (also known as z-score normalization) are relatively simple\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 11 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "model = LogisticRegression(random_state=0, penalty=\"none\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accurracy: 98.32402234636871%\n",
      "Testing Accurracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Since it is Classification Problem we compute Accuracy :D\n",
    "y_train_predict = model.predict(X_train)\n",
    "print(f\"Training Accurracy: {accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = model.predict(X_test)\n",
    "print(f\"Testing Accurracy: {accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #TODO: Implement sigmoid (hint: use np.exp)\n",
    "    # y= 1/(1+e^-x)\n",
    "    y=1/(1+np.exp(-1*x))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-1e2) = 3.7200759760208356e-44\n",
      "sigmoid(   0) = 0.5\n",
      "sigmoid(+1e2) = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(f\"{sigmoid(-1e2) = }\") # This should be almost equal 0\n",
    "print(f\"{sigmoid(   0) = }\") # This should be exactly 0.5\n",
    "print(f\"{sigmoid(+1e2) = }\") # This should be almost equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_accuracy_score(true, predicted):\n",
    "    #TODO: Implement an accuracy metric so that is can be used instead of Sklearn's accuracy score\n",
    "    #Note: both true and predicted will be boolean numpy array\n",
    "\n",
    "    # Accuracy metric to be using Total Correctly Classified Examples/ total no of Examples ðŸ˜‰\n",
    "    # Count no of correctly classified Examples\n",
    "    total_corrected=np.sum(true==predicted)\n",
    "\n",
    "    accuracy=total_corrected/np.shape(true)[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our_accuracy_score( np.array([True,  True]), np.array([True,  True]) ) = 1.0\n",
      "our_accuracy_score( np.array([True, False]), np.array([True,  True]) ) = 0.5\n",
      "our_accuracy_score( np.array([True, False]), np.array([True, False]) ) = 1.0\n",
      "our_accuracy_score( np.array([False, True]), np.array([True, False]) ) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(f\"{our_accuracy_score( np.array([True,  True]), np.array([True,  True]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True,  True]) ) = }\") # Should be 0.5\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True, False]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([False, True]), np.array([True, False]) ) = }\") # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any pre-made algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                           # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducibility\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multiplied by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "\n",
    "        # Convert True values to 1 and False values to -1\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "\n",
    "        # 1- sigmoid(x)= 1/(1+e^x)\n",
    "\n",
    "        N = np.shape(X)[0]\n",
    "        # Compute the dot product of W and X\n",
    "        z=X@self.w # 409,1\n",
    "        # Compute the logistic function of z\n",
    "        sigma_z=y*z # 409,1\n",
    "\n",
    "        # 1/(1+e^yWTX) = 1-sigmoid(yWTX)\n",
    "        # 1-sigmoid(sigma_z)\n",
    "        gradient_loss=(-1/N)*np.sum((y*X.T)*(1-sigmoid(sigma_z)),axis=1)  #(9,)\n",
    "\n",
    "        return gradient_loss\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "\n",
    "        # Weight update Eq: w(t+1)=w(t)-lr * grad_w\n",
    "        self.w=self.w - self.lr * self._gradient(X,y)\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "        for _ in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)\n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)\n",
    "\n",
    "        # Eq: p(y|X)=sigmoid(WTX)\n",
    "        sig=sigmoid(X@self.w)  #(batch_size,1) :D\n",
    "\n",
    "        return sig>self.probability_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to tune the hyper parameters\n",
    "def validate(lr, epochs):\n",
    "    validation_size = 0.3 #TODO: Choose a size for the validation set as a ratio from the training data\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=42)\n",
    "    # We will fit the model to only a subset of the training data and we will use the rest to evaluate the performance\n",
    "    our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_tr, y_tr)\n",
    "    # Then, we evaluate the performance using the validation set\n",
    "    return our_accuracy_score(y_val, our_model.predict(X_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2 epochs, the accuracy reaches 18.51851851851852% using lr=0.001\n",
      "In 10 epochs, the accuracy reaches 33.95061728395062% using lr=0.001\n",
      "In 50 epochs, the accuracy reaches 98.76543209876543% using lr=0.001\n",
      "In 100 epochs, the accuracy reaches 98.76543209876543% using lr=0.001\n",
      "In 500 epochs, the accuracy reaches 98.76543209876543% using lr=0.001\n"
     ]
    }
   ],
   "source": [
    "lr =0.001 #None #TODO: Choose a learning rate to use while testing different values for the number of epochs\n",
    "epochs_values = [2,10,50,100,500] #TODO: Choose a list of values for the number of epochs to test\n",
    "for epochs in epochs_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"In {epochs} epochs, the accuracy reaches {accuracy * 100}% using lr={lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- It is clear in case 50,100,500 epochs we get same Accuracy this means that the model has reaches his best performance can't learn more from looping more on the same example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr=0.5, the accuracy reaches 98.76543209876543% in 50 epochs\n",
      "Using lr=0.1, the accuracy reaches 98.76543209876543% in 50 epochs\n",
      "Using lr=0.01, the accuracy reaches 98.76543209876543% in 50 epochs\n",
      "Using lr=0.001, the accuracy reaches 98.76543209876543% in 50 epochs\n",
      "Using lr=1e-05, the accuracy reaches 17.901234567901234% in 50 epochs\n"
     ]
    }
   ],
   "source": [
    "epochs = 50 #TODO: Choose the number of epochs to use while testing different values for the learning rate\n",
    "lr_values = [0.5,0.1,0.01,0.001,0.00001] #TODO: Choose a list of values for the learning rate to test\n",
    "for lr in lr_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"Using lr={lr}, the accuracy reaches {accuracy * 100}% in {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- For very small learning rate 0.00001 we got very small accuracy because update rule is very small converging to thr local min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = our_model.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = our_model.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your conclusion about your implementation's performance and training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-Learn:\n",
    "```\n",
    "    CPU times: total: 0 ns\n",
    "    Wall time: 11 ms\n",
    "\n",
    "    Training Accurracy: 98.32402234636871%\n",
    "    Testing Accurracy: 96.53679653679653%\n",
    "```\n",
    "\n",
    "Our Logistic Regression: [lr 0.001 50 Epoch]\n",
    "```\n",
    "    CPU times: total: 15.6 ms\n",
    "    Wall time: 4.01 ms\n",
    "    \n",
    "    Training Accuracy: 98.32402234636871%\n",
    "    Testing Accuracy: 96.53679653679653%\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "We Achieved Same Accuracy as Scikit-Learn but we have more execution time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "As a bonus, you can implement and test the following:\n",
    "* Stochastic gradient descent\n",
    "* Termination conditions (e.g. The gradient check)\n",
    "  \n",
    "Write your conclusion about any results you calculate for your bonus implementations.\n",
    "\n",
    "**IMPORTANT**: Do not implement the bonus in the previous cells. You can copy and paste codes from the previous cells and continue your implementation below this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any pre-made algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegressionSGD:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None,convergence_tol = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                           # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducibility\n",
    "\n",
    "\n",
    "        # Converge Condition between losses of 2 successive epochs\n",
    "        self.convergence_tol = convergence_tol\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multiplied by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "\n",
    "        # Convert True values to 1 and False values to -1\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _stochastic_gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "\n",
    "        # 1- sigmoid(x)= 1/(1+e^x)\n",
    "\n",
    "        # Compute the dot product of W and X\n",
    "        z=X@self.w # 1,1\n",
    "        # Compute the logistic function of z\n",
    "        sigma_z=y*z # 1,1\n",
    "\n",
    "        # # 1/(1+e^yWTX) = 1-sigmoid(yWTX)\n",
    "        # # 1-sigmoid(sigma_z)\n",
    "        gradient_loss=-1*(y*X.T)*(1-sigmoid(sigma_z))  #(9,)\n",
    "\n",
    "        return gradient_loss\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "        N=X.shape[0]\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        indices = np.random.permutation(N)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(N):\n",
    "            # UPDATE Weights With Every Training Example\n",
    "            # Weight update Eq: w(t+1)=w(t)-lr * grad_w\n",
    "            self.w=self.w - self.lr * self._stochastic_gradient(X_shuffled[i],y_shuffled[i])\n",
    "\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "\n",
    "\n",
    "\n",
    "        prev_loss = np.inf \n",
    "        for epoch in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)\n",
    "\n",
    "            if self.convergence_tol:\n",
    "                # Calculate the loss for the current epoch\n",
    "                current_loss = self._loss(X, y)\n",
    "                # Check for convergence: stop if the change in loss is below the tolerance\n",
    "                if np.abs(current_loss - prev_loss) < self.convergence_tol:\n",
    "                    print(f\"Convergence reached at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "                prev_loss = current_loss  # Update previous loss for the next iteration\n",
    "\n",
    "\n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)\n",
    "\n",
    "        # Eq: p(y|X)=sigmoid(WTX)\n",
    "        sig=sigmoid(X@self.w)  #(batch_size,1) :D\n",
    "\n",
    "        return sig>self.probability_threshold\n",
    "    \n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        # Calculate the raw scores (logits)\n",
    "        z = X @ self.w\n",
    "        # Apply the sigmoid function to obtain probabilities\n",
    "        prob = sigmoid(z)\n",
    "        # Calculate the binary cross-entropy loss\n",
    "        loss = -np.mean(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 266 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "our_model_SGD = OurLogisticRegressionSGD(lr=lr, epochs=epochs, random_state=0,convergence_tol = None).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "y_train_predict_SGD = our_model_SGD.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict_SGD) * 100}%\")\n",
    "y_test_predict_SGD = our_model_SGD.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict_SGD) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached at epoch 6\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 61 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "our_model_SGD = OurLogisticRegressionSGD(lr=lr, epochs=epochs, random_state=0,convergence_tol = 1e-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "y_train_predict_SGD = our_model_SGD.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict_SGD) * 100}%\")\n",
    "y_test_predict_SGD = our_model_SGD.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict_SGD) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Same Accuracy is reached but Longer Time I think this is due to updating with every one example so More Steps are required for Converge Since Weight is adjusted based on one example at a time so adjusting it to one example may cause it is to misclassify previously adjusted examples\n",
    "- The in between Solution is Mini- Batch Gradient Descent\n",
    "\n",
    "Without Early Stopping\n",
    "```\n",
    "CPU times: total: 266 ms\n",
    "Wall time: 272 ms\n",
    "```\n",
    "After Adding Early Stopping by checking on loss \n",
    "```\n",
    "Convergence reached at epoch 6\n",
    "CPU times: total: 31.2 ms\n",
    "Wall time: 33 ms\n",
    "```\n",
    "\n",
    "\n",
    "#### Reduction in Time ðŸ˜‹ with same accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd780a10ad03a506e232ec29f104692e8d999a77309c0fc915217df500c72051"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
