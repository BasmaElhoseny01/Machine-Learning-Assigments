{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "w=np.array([[1],[2],[3]])  \n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "y=np.array([[1],[-1],[1],[-1],[-1]])  \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[1,2,3],[1,2,83],[1,22,3],[1,2,-3],[10,20,30]])\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14]\n",
      " [254]\n",
      " [ 54]\n",
      " [ -4]\n",
      " [140]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=x@w\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3],\n",
       "       [ -1,  -2, -83],\n",
       "       [  1,  22,   3],\n",
       "       [ -1,  -2,   3],\n",
       "       [-10, -20, -30]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  14],\n",
       "       [-254],\n",
       "       [  54],\n",
       "       [   4],\n",
       "       [-140]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y*x@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07142857,  0.14285714,  0.21428571],\n",
       "       [ 0.00393701,  0.00787402,  0.32677165],\n",
       "       [ 0.01851852,  0.40740741,  0.05555556],\n",
       "       [-0.25      , -0.5       ,  0.75      ],\n",
       "       [ 0.07142857,  0.14285714,  0.21428571]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y*x )/(y*x@w)  # (5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08468733,  0.20099571,  1.56089864])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((y*x )/(y*x@w) ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any pre-made algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                           # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducibility\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multiplied by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "\n",
    "        # Convert True values to 1 and False values to -1\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "\n",
    "        # 1- sigmoid(x)= 1/(1+e^x)\n",
    "\n",
    "        N = np.shape(X)[0]\n",
    "        # # Compute the dot product of W and X\n",
    "        # z = np.dot(X, self.w)\n",
    "\n",
    "        # # Compute the logistic function of z\n",
    "        # sigma_z = sigmoid(-1*y * z)\n",
    "        # print(sigma_z.shape)\n",
    "\n",
    "        # y_x=(X.T * y).T #X  (402,9) y(402,)[Broadcasted] = (9, 402).T=(402,9)\n",
    "\n",
    "        z= X@self.w.T  # (402,9)*(9,1) = (402, 1)\n",
    "        sigma_z = sigmoid(y*z) #(402,1) (1/1+e^(yWtX))\n",
    "\n",
    "        # (9, 402) / (402, 1) = \n",
    "        # (y * X.T / (1 + sigma_z) # (9,402)\n",
    "        loss=(-1/N)*np.sum(y * X.T / (1 + sigma_z),axis=1)  #(9,)\n",
    "\n",
    "\n",
    "        gradient_loss=(-1/N)*np.sum(  *(1-sigmoid(  )))  #(9,)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "\n",
    "        # Weight update Eq: w(t+1)=w(t)-lr * grad_w\n",
    "        self.w=self.w - self.lr * self._gradient(X,y)\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "        for _ in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)\n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)\n",
    "\n",
    "        # Eq: p(y|X)=sigmoid(WTX)\n",
    "        sig=sigmoid(X@self.w)  #(batch_size,1) :D\n",
    "\n",
    "        return sig>self.probability_threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
